# å¿«é€Ÿå‚è€ƒæ‰‹å†Œä¸ç´¢å¼•

## 1. å¿«é€Ÿå‚è€ƒæ¸…å•

### 1.1 ğŸš€ ç«‹å³è¡ŒåŠ¨æ¸…å•

#### å…³é”®ä¼˜åŒ–ï¼ˆå¿…é¡»åšï¼‰
```bash
# æ£€æŸ¥å¹¶ç§»é™¤æ€§èƒ½æ€æ‰‹å‚æ•°
grep "enforce-eager" å¯åŠ¨è„šæœ¬  # å¦‚æœæœ‰ï¼Œç«‹å³åˆ é™¤

# å¯ç”¨æœ€ä¼˜é…ç½®
--dtype bfloat16 --kv-cache-dtype "fp8"

# è°ƒæ•´å†…å­˜åˆ©ç”¨ç‡
--gpu-memory-utilization 0.90
```

#### åŸºç¡€éªŒè¯ï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼‰
```bash
# æœåŠ¡çŠ¶æ€æ£€æŸ¥
curl -f http://localhost:8200/health

# æ€§èƒ½åŸºå‡†æµ‹è¯•
curl -X POST http://localhost:8200/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "Qwen3-VL-235B-A22B-Instruct", "prompt": "æµ‹è¯•", "max_tokens": 50}' --time

# GPUçŠ¶æ€æ£€æŸ¥
nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used,memory.total,temperature.gpu --format=csv
```

### 1.2 ğŸ“Š æ€§èƒ½æŒ‡æ ‡é€ŸæŸ¥è¡¨

#### åŸºå‡†æ€§èƒ½ï¼ˆB200Ã—8 + Qwen3-VL-235Bï¼‰
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é…ç½®çº§åˆ«       â”‚ ååé‡   â”‚ P50å»¶è¿Ÿ  â”‚ P95å»¶è¿Ÿ  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å·®é…ç½®         â”‚ ~800     â”‚ 5.2s     â”‚ 15s      â”‚
â”‚ åŸºçº¿é…ç½®       â”‚ ~1200    â”‚ 4.0s     â”‚ 12s      â”‚
â”‚ ä¼˜åŒ–é…ç½®       â”‚ ~2000    â”‚ 1.4s     â”‚ 8s       â”‚
â”‚ æè‡´é…ç½®       â”‚ ~2500    â”‚ 1.0s     â”‚ 6s       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å‘Šè­¦é˜ˆå€¼ï¼ˆå¿«é€Ÿåˆ¤æ–­ï¼‰
```
æ­£å¸¸:     ååé‡>1800 tok/s, P50<2s, GPU>80%
è­¦å‘Š:     ååé‡ä¸‹é™>20%, P50>4s, GPU<70%
ä¸¥é‡:     ååé‡ä¸‹é™>50%, P50>8s, OOMé”™è¯¯
```

### 1.3 ğŸ› ï¸ é…ç½®æ¨¡æ¿é€ŸæŸ¥

#### ç”Ÿäº§ç¯å¢ƒé…ç½®
```bash
python3 -m vllm.entrypoints.openai.api_server \
  --model /model-cache/Qwen/Qwen3-VL-235B-A22B-Instruct \
  --served-model-name Qwen3-VL-235B-A22B-Instruct \
  --host 0.0.0.0 --port 8200 \
  --tensor-parallel-size 4 \
  --max-model-len 262144 \
  --enable-chunked-prefill \
  --max-num-batched-tokens 262144 \
  --gpu-memory-utilization 0.90 \
  --max-num-seqs 96 \
  --dtype bfloat16 \
  --kv-cache-dtype "fp8" \
  --trust-remote-code
  # å…³é”®ï¼šä¸è¦åŠ  --enforce-eager
```

#### å¼€å‘ç¯å¢ƒé…ç½®
```bash
# åœ¨ç”Ÿäº§é…ç½®åŸºç¡€ä¸Šä¿®æ”¹ï¼š
--gpu-memory-utilization 0.80 \
--max-num-seqs 32 \
--log-level DEBUG \
# è°ƒè¯•æ—¶å¯åŠ ï¼š--enforce-eager
```

#### é«˜å»¶è¿Ÿåœºæ™¯é…ç½®
```bash
# åœ¨ç”Ÿäº§é…ç½®åŸºç¡€ä¸Šä¿®æ”¹ï¼š
--max-num-seqs 32 \
--max-num-batched-tokens 65536 \
--gpu-memory-utilization 0.85
```

## 2. æ•…éšœæ’é™¤é€ŸæŸ¥è¡¨

### 2.1 ğŸš¨ ç´§æ€¥æ•…éšœå¿«é€Ÿå“åº”

#### OOMæ•…éšœ
```bash
# ç«‹å³æªæ–½
1. é™ä½å¹¶å‘ï¼š--max-num-seqs 32
2. é™ä½å†…å­˜ï¼š--gpu-memory-utilization 0.80
3. é‡å¯æœåŠ¡ï¼špkill -f vllm
4. æ¸…ç†GPUï¼šnvidia-smi --gpu-reset
```

#### æ€§èƒ½éª¤é™
```bash
# æ£€æŸ¥æ¸…å•
â–¡ é…ç½®ä¸­æ˜¯å¦åŒ…å« --enforce-eager
â–¡ GPUæ˜¯å¦é™é¢‘ï¼ˆæ¸©åº¦è¿‡é«˜ï¼‰
â–¡ ç½‘ç»œå»¶è¿Ÿæ˜¯å¦å¼‚å¸¸
â–¡ æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
```

#### æœåŠ¡ä¸å¯ç”¨
```bash
# å¿«é€Ÿè¯Šæ–­
netstat -tlnp | grep 8200  # ç«¯å£æ£€æŸ¥
ps aux | grep vllm           # è¿›ç¨‹æ£€æŸ¥
nvidia-smi                   # GPUæ£€æŸ¥
curl http://localhost:8200/health  # å¥åº·æ£€æŸ¥
```

### 2.2 ğŸ”§ å¸¸è§é—®é¢˜å¿«é€Ÿè§£å†³

| é—®é¢˜ | å¿«é€Ÿè¯Šæ–­ | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|
| ååé‡ä½ | æ£€æŸ¥--enforce-eagerå‚æ•° | ç§»é™¤è¯¥å‚æ•° |
| å»¶è¿Ÿé«˜ | æ£€æŸ¥max-num-seqsè®¾ç½® | é€‚å½“å‡å°‘æˆ–å¢åŠ  |
| OOMé¢‘ç¹ | æ£€æŸ¥å†…å­˜åˆ©ç”¨ç‡è®¾ç½® | é™ä½åˆ°0.85 |
| GPUåˆ©ç”¨ç‡ä½ | æ£€æŸ¥æ‰¹å¤„ç†å¤§å° | å¢åŠ max-num-batched-tokens |
| å¯åŠ¨å¤±è´¥ | æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæƒé™ | ç¡®è®¤è·¯å¾„æ­£ç¡®å’Œæƒé™è¶³å¤Ÿ |

### 2.3 ğŸ“ˆ æ€§èƒ½è°ƒä¼˜é€ŸæŸ¥

#### ååé‡ä¼˜åŒ–
```
æ­£å‘ï¼šå¢åŠ max-num-seqs, å¢åŠ batched-tokens, å¯ç”¨FP8
è´Ÿå‘ï¼šè¿‡å¤§çš„å¹¶å‘å¯èƒ½å¯¼è‡´æ’é˜Ÿï¼Œéœ€è¦å¹³è¡¡
```

#### å»¶è¿Ÿä¼˜åŒ–
```
TTFTï¼šå¯ç”¨chunked-prefill, å‡å°‘batch size
TPOTï¼šæå‡GPUåˆ©ç”¨ç‡, å¯ç”¨CUDA Graph
P95ï¼šä¼˜åŒ–å¹¶å‘å‚æ•°, é¿å…èµ„æºç«äº‰
```

#### æˆæœ¬ä¼˜åŒ–
```
å†…å­˜ï¼šå¯ç”¨FP8é‡åŒ–, æå‡GPUåˆ©ç”¨ç‡
ç¡¬ä»¶ï¼šåˆç†å¹¶è¡Œç­–ç•¥, æ™ºèƒ½è°ƒåº¦
è¿ç»´ï¼šè‡ªåŠ¨åŒ–ç›‘æ§, é¢„é˜²æ€§ç»´æŠ¤
```

## 3. å‚æ•°é…ç½®é€ŸæŸ¥è¡¨

### 3.1 ğŸ”´ å…³é”®å‚æ•°å½±å“çŸ©é˜µ

| å‚æ•° | å½±å“ | æ¨èå€¼ | é”™è¯¯å€¼ | æ€§èƒ½å½±å“ |
|------|------|--------|--------|----------|
| enforce-eager | CUDA Graph | ä¸ä½¿ç”¨ | ä½¿ç”¨ | -153% |
| gpu-memory-utilization | å†…å­˜ä½¿ç”¨ | 0.90 | >0.95 | OOMé£é™© |
| dtype | è®¡ç®—ç²¾åº¦ | bfloat16 | float32 | -50% |
| kv-cache-dtype | å†…å­˜å ç”¨ | fp8 | fp16 | +100%å†…å­˜ |
| max-num-seqs | å¹¶å‘æ•° | 96 | 32 | -20%åå |
| tensor-parallel-size | å¹¶è¡Œåº¦ | 4 | 1 | -75% |

### 3.2 âš¡ ä¼˜åŒ–å‚æ•°ç»„åˆ

#### ååé‡ä¼˜å…ˆ
```bash
--gpu-memory-utilization 0.95 \
--max-num-seqs 128 \
--max-num-batched-tokens 524288 \
--kv-cache-dtype "fp8" \
--async-scheduling
```

#### å»¶è¿Ÿä¼˜å…ˆ
```bash
--max-num-seqs 32 \
--max-num-batched-tokens 65536 \
--enable-chunked-prefill \
--gpu-memory-utilization 0.85
```

#### ç¨³å®šæ€§ä¼˜å…ˆ
```bash
--gpu-memory-utilization 0.80 \
--max-num-seqs 64 \
--kv-cache-dtype "auto" \
--log-level INFO
```

### 3.3 ğŸ›ï¸ ç¡¬ä»¶é€‚é…é…ç½®

#### B200/A100ï¼ˆç°ä»£GPUï¼‰
```bash
--dtype bfloat16 \
--kv-cache-dtype "fp8" \
--tensor-parallel-size 4 \
--gpu-memory-utilization 0.90-0.95
```

#### V100/T4ï¼ˆè€æ—§GPUï¼‰
```bash
--dtype float16 \
--kv-cache-dtype "auto" \
--tensor-parallel-size 2 \
--gpu-memory-utilization 0.80-0.85
```

## 4. ç›‘æ§æŒ‡æ ‡é€ŸæŸ¥

### 4.1 ğŸ“Š ä¸šåŠ¡æŒ‡æ ‡

| æŒ‡æ ‡ | æ­£å¸¸èŒƒå›´ | è­¦å‘Š | ä¸¥é‡ |
|------|----------|-------|------|
| ååé‡ | >1800 tok/s | <1500 | <1000 |
| P50å»¶è¿Ÿ | <2s | >3s | >5s |
| P95å»¶è¿Ÿ | <8s | >12s | >20s |
| æˆåŠŸç‡ | >99% | <98% | <95% |

### 4.2 ğŸ”§ ç³»ç»ŸæŒ‡æ ‡

| æŒ‡æ ‡ | æ­£å¸¸èŒƒå›´ | è­¦å‘Š | ä¸¥é‡ |
|------|----------|-------|------|
| GPUåˆ©ç”¨ç‡ | 80-95% | 70-80% | <70% |
| GPUå†…å­˜ | 70-90% | >95% | 100% |
| GPUæ¸©åº¦ | <75Â°C | 75-85Â°C | >85Â°C |
| CPUåˆ©ç”¨ç‡ | 60-80% | >90% | >95% |

### 4.3 ğŸš¨ å‘Šè­¦é€ŸæŸ¥

#### Criticalå‘Šè­¦ï¼ˆç«‹å³å“åº”ï¼‰
```
- æœåŠ¡ä¸å¯ç”¨ï¼ˆ5xxé”™è¯¯ >5%ï¼‰
- GPUå†…å­˜OOM
- ååé‡ä¸‹é™ >50%
- P95å»¶è¿Ÿ >15s
```

#### Warningå‘Šè­¦ï¼ˆ1å°æ—¶å†…å“åº”ï¼‰
```
- ååé‡ä¸‹é™ >20%
- GPUåˆ©ç”¨ç‡ <70%
- P50å»¶è¿Ÿ >4s
- é”™è¯¯ç‡ >3%
```

## 5. å‘½ä»¤è¡Œå·¥å…·é€ŸæŸ¥

### 5.1 ğŸ” è¯Šæ–­å‘½ä»¤

```bash
# åŸºç¡€è¯Šæ–­
nvidia-smi                          # GPUçŠ¶æ€
ps aux | grep vllm                 # è¿›ç¨‹çŠ¶æ€
netstat -tlnp | grep 8200          # ç«¯å£çŠ¶æ€
free -h                           # å†…å­˜ä½¿ç”¨
df -h /model-cache                 # ç£ç›˜ç©ºé—´

# æ€§èƒ½æµ‹è¯•
curl -w "@curl-format.txt" -o /dev/null -s "http://localhost:8200/health"
stress --cpu 4 --timeout 60s        # CPUå‹åŠ›æµ‹è¯•
```

### 5.2 ğŸ“ˆ ç›‘æ§å‘½ä»¤

```bash
# å®æ—¶ç›‘æ§
watch -n 1 'nvidia-smi --query-gpu=utilization.gpu,utilization.memory,temperature.gpu --format=csv,noheader'
tail -f /var/log/vllm/server.log

# ç½‘ç»œç›‘æ§
iftop -i eth0                    # ç½‘ç»œæµé‡
ss -tulnp | grep 8200              # è¿æ¥çŠ¶æ€

# ç³»ç»Ÿèµ„æº
htop                             # è¿›ç¨‹ç›‘æ§
iotop                            # IOç›‘æ§
```

### 5.3 ğŸ› ï¸ ç»´æŠ¤å‘½ä»¤

```bash
# æœåŠ¡ç®¡ç†
systemctl status vllm               # æœåŠ¡çŠ¶æ€
systemctl restart vllm             # é‡å¯æœåŠ¡
systemctl reload vllm              # é‡è½½é…ç½®

# æ¸…ç†æ“ä½œ
pkill -f vllm                    # å¼ºåˆ¶åœæ­¢
nvidia-smi --gpu-reset            # é‡ç½®GPU
rm -rf /tmp/vllm-*              # æ¸…ç†ä¸´æ—¶æ–‡ä»¶

# æ—¥å¿—ç®¡ç†
journalctl -u vllm -f            # æŸ¥çœ‹æ—¥å¿—
logrotate -f /etc/logrotate.d/vllm # è½®è½¬æ—¥å¿—
```

## 6. é…ç½®æ–‡ä»¶é€ŸæŸ¥

### 6.1 ğŸ“‹ ç¯å¢ƒå˜é‡é…ç½®

```bash
# å¿…éœ€ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3
export NCCL_DEBUG=INFO
export NCCL_SOCKET_IFNAME=eth0

# å¯é€‰ä¼˜åŒ–
export OMP_NUM_THREADS=8
export CUDA_LAUNCH_BLOCKING=0
export PYTHONPATH=$PYTHONPATH:/opt/vllm
```

### 6.2 ğŸ³ Dockeré…ç½®

```yaml
# ç”Ÿäº§ç¯å¢ƒDocker Compose
version: '3.8'
services:
  vllm:
    image: vllm/vllm-openai:latest
    environment:
      - CUDA_VISIBLE_DEVICES=0,1,2,3
    volumes:
      - /model-cache:/model-cache
    ports:
      - "8200:8200"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 4
              capabilities: [gpu]
```

### 6.3 âš™ï¸ Nginxé…ç½®

```nginx
upstream vllm_backend {
    server 127.0.0.1:8200 max_fails=3 fail_timeout=30s;
}

server {
    listen 443 ssl http2;
    
    location / {
        proxy_pass http://vllm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
    }
    
    location /health {
        proxy_pass http://vllm_backend/health;
        access_log off;
    }
}
```

## 7. æ–‡æ¡£ç´¢å¼•

### 7.1 ğŸ“š æ ¸å¿ƒæ–‡æ¡£å¯¼èˆª

```
ai-inference-optimization-guide/
â”œâ”€â”€ README.md                     # æ€»ä½“ä»‹ç»
â”œâ”€â”€ 01-architecture-overview.md    # æ¶æ„æ¦‚è¿°
â”œâ”€â”€ 02-performance-analysis.md     # æ€§èƒ½åˆ†æ
â”œâ”€â”€ 03-optimization-strategy.md    # ä¼˜åŒ–ç­–ç•¥
â”œâ”€â”€ 04-configuration-guide.md     # é…ç½®æŒ‡å—
â”œâ”€â”€ 05-troubleshooting.md         # æ•…éšœæ’é™¤
â”œâ”€â”€ 06-best-practices.md         # æœ€ä½³å®è·µ
â”œâ”€â”€ 07-quick-reference.md         # å¿«é€Ÿå‚è€ƒï¼ˆæœ¬æ–‡æ¡£ï¼‰
â””â”€â”€ data/
    â””â”€â”€ test-results.md           # æµ‹è¯•æ•°æ®
```

### 7.2 ğŸ¯ æŒ‰è§’è‰²æŸ¥æ‰¾ä¿¡æ¯

#### æ¶æ„å¸ˆ
- ç³»ç»Ÿè®¾è®¡ï¼š01-architecture-overview.md
- æ€§èƒ½è§„åˆ’ï¼š02-performance-analysis.md
- ä¼˜åŒ–ç­–ç•¥ï¼š03-optimization-strategy.md

#### å·¥ç¨‹å¸ˆ
- é…ç½®å‚æ•°ï¼š04-configuration-guide.md
- æ•…éšœå¤„ç†ï¼š05-troubleshooting.md
- æœ€ä½³å®è·µï¼š06-best-practices.md

#### è¿ç»´å·¥ç¨‹å¸ˆ
- ç›‘æ§ä½“ç³»ï¼š05-troubleshooting.md
- è¿ç»´æµç¨‹ï¼š06-best-practices.md
- å¿«é€Ÿå‚è€ƒï¼š07-quick-reference.md

### 7.3 ğŸ·ï¸ æŒ‰é—®é¢˜ç±»å‹æŸ¥æ‰¾

#### æ€§èƒ½é—®é¢˜
- ååé‡ä½ï¼š02-performance-analysis.md#æ€§èƒ½ç“¶é¢ˆåˆ†æ
- å»¶è¿Ÿé«˜ï¼š02-performance-analysis.md#å»¶è¿Ÿä¼˜åŒ–
- OOMé—®é¢˜ï¼š05-troubleshooting.md#oomæ•…éšœ

#### é…ç½®é—®é¢˜
- å‚æ•°è¯´æ˜ï¼š04-configuration-guide.md
- ç¯å¢ƒé…ç½®ï¼š06-best-practices.md#é…ç½®ç®¡ç†
- æ•…éšœæ’é™¤ï¼š05-troubleshooting.md#å¸¸è§æ•…éšœæ¨¡å¼

#### éƒ¨ç½²é—®é¢˜
- å®¹å™¨åŒ–ï¼š06-best-practices.md#å®¹å™¨åŒ–éƒ¨ç½²
- ç½‘ç»œé…ç½®ï¼š05-troubleshooting.md#åˆ†å¸ƒå¼é€šä¿¡æ•…éšœ
- è´Ÿè½½å‡è¡¡ï¼š06-best-practices.md#é«˜å¯ç”¨æ¶æ„

### 7.4 ğŸ“– æ‰©å±•é˜…è¯»

#### å®˜æ–¹æ–‡æ¡£
- [vLLMå®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [CUDAç¼–ç¨‹æŒ‡å—](https://docs.nvidia.com/cuda/)
- [NVIDIA NCCLæ–‡æ¡£](https://docs.nvidia.com/deeplearning/nccl/)

#### æ€§èƒ½ä¼˜åŒ–
- [CUDAæ€§èƒ½æœ€ä½³å®è·µ](https://developer.nvidia.com/blog/cuda-performance-tips/)
- [å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–](https://huggingface.co/docs/transformers/performance)
- [FP8é‡åŒ–æŒ‡å—](https://docs.nvidia.com/deeplearning/transformer-engine/)

#### ç›‘æ§å·¥å…·
- [Prometheusç›‘æ§æŒ‡å—](https://prometheus.io/docs/)
- [Grafanaå¯è§†åŒ–](https://grafana.com/docs/)
- [DCGMç›‘æ§](https://developer.nvidia.com/dcgm)

---

## å¿«é€Ÿä½¿ç”¨æŒ‡å—

### æ–°æ‰‹å…¥é—¨ï¼ˆ5åˆ†é’Ÿï¼‰
1. é˜…è¯» `README.md` äº†è§£æ€»ä½“æ¦‚å¿µ
2. æŸ¥çœ‹ `07-quick-reference.md` #1.1 ç«‹å³è¡ŒåŠ¨æ¸…å•
3. ä½¿ç”¨é…ç½®æ¨¡æ¿å¿«é€Ÿéƒ¨ç½²

### é—®é¢˜æ’æŸ¥ï¼ˆ10åˆ†é’Ÿï¼‰
1. æŸ¥çœ‹ `07-quick-reference.md` #2.1 ç´§æ€¥æ•…éšœå¿«é€Ÿå“åº”
2. å‚è€ƒ `05-troubleshooting.md` è¯¦ç»†æ•…éšœå¤„ç†
3. ä½¿ç”¨è¯Šæ–­å‘½ä»¤å¿«é€Ÿå®šä½é—®é¢˜

### æ€§èƒ½ä¼˜åŒ–ï¼ˆ30åˆ†é’Ÿï¼‰
1. é˜…è¯» `02-performance-analysis.md` ç†è§£æ€§èƒ½ç“¶é¢ˆ
2. å‚è€ƒ `03-optimization-strategy.md` åˆ¶å®šä¼˜åŒ–ç­–ç•¥
3. ä½¿ç”¨ `04-configuration-guide.md` è°ƒæ•´å‚æ•°é…ç½®

### ç”Ÿäº§éƒ¨ç½²ï¼ˆ1å°æ—¶ï¼‰
1. å­¦ä¹  `06-best-practices.md` æœ€ä½³å®è·µ
2. å‚è€ƒæ¡ˆä¾‹ç ”ç©¶åˆ¶å®šéƒ¨ç½²æ–¹æ¡ˆ
3. å»ºç«‹ç›‘æ§å’Œè¿ç»´ä½“ç³»

è¿™ä¸ªå¿«é€Ÿå‚è€ƒæ‰‹å†Œä¸ºæ—¥å¸¸ä½¿ç”¨æä¾›äº†å³æŸ¥å³ç”¨çš„æŒ‡å¯¼ï¼Œæ˜¯æ•´ä¸ªçŸ¥è¯†ä½“ç³»çš„å®ç”¨ç´¢å¼•ã€‚

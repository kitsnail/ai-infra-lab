# llm-d å¸¸è§æ•…éšœæ’æŸ¥æŒ‡å—

## ğŸš¨ å¿«é€Ÿè¯Šæ–­

```bash
# åŸºç¡€çŠ¶æ€æ£€æŸ¥
kubectl get pods -n llm-d
kubectl get svc -n llm-d
kubectl get events -n llm-d --sort-by=.metadata.creationTimestamp | tail -10

# å…³é”®æ—¥å¿—æ£€æŸ¥
kubectl logs -f <epp-pod> | grep "Request handled"
kubectl logs -f <vllm-pod> | grep -i error
kubectl logs -f body-based-router-xxx | grep -E "(Processed|error)"
```

## ğŸ“‹ éƒ¨ç½²ç±»é—®é¢˜

### Podå¯åŠ¨å¤±è´¥

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| **ImagePullBackOff** | é•œåƒæ‹‰å–å¤±è´¥ | æ£€æŸ¥ç½‘ç»œã€é•œåƒä»“åº“æƒé™ |
| **CrashLoopBackOff** | é…ç½®é”™è¯¯æˆ–èµ„æºä¸è¶³ | æ£€æŸ¥ConfigMapã€èµ„æºé™åˆ¶ |
| **Pending** | è°ƒåº¦å¤±è´¥ | æ£€æŸ¥èµ„æºé…é¢ã€æ±¡ç‚¹å®¹å¿åº¦ |

### helmwaveéƒ¨ç½²é—®é¢˜ (v1.3.1)

```bash
# æ£€æŸ¥helmwaveçŠ¶æ€
helmwave status

# é‡æ–°æ„å»ºéƒ¨ç½²
helmwave up --build --force

# æ£€æŸ¥ä¾èµ–
helmwave deps
```

### PVCå­˜å‚¨é—®é¢˜

```bash
# æ£€æŸ¥PVCçŠ¶æ€
kubectl get pvc -n llm-d
kubectl describe pvc llm-model -n llm-d

# æ£€æŸ¥æ¨¡å‹ä¸‹è½½
kubectl exec -it <model-pod> -- ls -la /model-cache
```

## ğŸŒ ç½‘ç»œç±»é—®é¢˜

### æœåŠ¡å‘ç°å¤±è´¥

```bash
# æµ‹è¯•å†…éƒ¨è¿æ¥
kubectl exec -it <pod-name> -- nslookup <service-name>.llm-d.svc.cluster.local

# æ£€æŸ¥Endpoints
kubectl get endpoints -n llm-d
```

### Gatewayè·¯ç”±é—®é¢˜

```bash
# æ£€æŸ¥Gatewayé…ç½®
kubectl get gateway -n llm-d
kubectl get httproute -n llm-d -o yaml

# æ£€æŸ¥Envoyé…ç½®
kubectl exec -it <istio-gateway> -- curl localhost:15000/config_dump
```

### ExtProcé“¾é—®é¢˜

| é—®é¢˜ | æ£€æŸ¥æ–¹æ³• | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|
| **BBRæ— å“åº”** | `kubectl logs body-based-router | grep error` | æ£€æŸ¥Service discovery |
| **EPPè·¯ç”±å¤±è´¥** | `kubectl logs gaie-xxx-epp | grep "Request handled"` | éªŒè¯æ’ä»¶é…ç½® |
| **é“¾å¼é¡ºåºé”™è¯¯** | æ£€æŸ¥EnvoyFilter patché¡ºåº | ç¡®ä¿BBRåœ¨å‰ï¼ŒEPPåœ¨å |

## ğŸ¤– æ¨¡å‹æ¨ç†é—®é¢˜

### æ¨¡å‹åŠ è½½å¤±è´¥

```bash
# æ£€æŸ¥åŠ è½½æ—¥å¿—
kubectl logs <vllm-pod> | grep -E "(Loading|Error|Failed)"

# æ£€æŸ¥HF Token
kubectl get secret llm-d-hf-token -o yaml

# æ£€æŸ¥GPUçŠ¶æ€
kubectl exec -it <vllm-pod> -- nvidia-smi
```

### æ€§èƒ½é—®é¢˜

| æŒ‡æ ‡ | é—®é¢˜è¯Šæ–­ | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|
| **TTFT > 5s** | æ¨¡å‹é¢„åŠ è½½æˆ–ç¼“å­˜é—®é¢˜ | æ£€æŸ¥é¢„çƒ­é…ç½®ï¼Œå¯ç”¨PDåˆ†ç¦» |
| **TPOT > 200ms** | æ‰¹å¤„ç†æˆ–GPUåˆ©ç”¨ç‡ä½ | ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°ï¼Œæ£€æŸ¥GPUä½¿ç”¨ |
| **ç¼“å­˜å‘½ä¸­ç‡ < 10%** | prefix-cache-scoreræœªå¯ç”¨ | åˆ‡æ¢åˆ°V2æ’ä»¶ |

## ğŸ’¾ ç¼“å­˜ç³»ç»Ÿé—®é¢˜

### KVç¼“å­˜é—®é¢˜

```bash
# æ£€æŸ¥ç¼“å­˜çŠ¶æ€
kubectl exec -it <kv-cache-pod> -- curl localhost:8080/cache/status

# æ£€æŸ¥ç¼“å­˜æŒ‡æ ‡
kubectl port-forward <kv-cache-pod> 9090:9090
curl http://localhost:9090/metrics | grep cache_hit_ratio
```

### ç¼“å­˜ä¼˜åŒ–æ£€æŸ¥

| é—®é¢˜ | ä¼˜åŒ–å»ºè®® |
|------|----------|
| **å‘½ä¸­ç‡ä½** | è°ƒæ•´block_sizeå’Œmax_prefix_blocks_to_match |
| **å†…å­˜è¿‡é«˜** | å‡å°‘cacheå®¹é‡æˆ–è°ƒæ•´gc_threshold |
| **åŒæ­¥å¤±è´¥** | æ£€æŸ¥ç½‘ç»œè¿é€šæ€§å’Œæƒé™é…ç½® |

## ğŸ› ï¸ æ•…éšœæ’æŸ¥å·¥å…·

### è¯Šæ–­è„šæœ¬

```bash
#!/bin/bash
# llm-d-diagnostics.sh
NAMESPACE=${1:-llm-d}

echo "=== llm-d è¯Šæ–­æŠ¥å‘Š ==="
echo "æ—¶é—´: $(date)"
echo ""

# PodçŠ¶æ€
echo "=== PodçŠ¶æ€ ==="
kubectl get pods -n $NAMESPACE -o wide

# æœåŠ¡çŠ¶æ€
echo "=== æœåŠ¡çŠ¶æ€ ==="
kubectl get svc -n $NAMESPACE

# èµ„æºä½¿ç”¨
echo "=== èµ„æºä½¿ç”¨ ==="
kubectl top pods -n $NAMESPACE --sort-by=cpu

# æœ€è¿‘äº‹ä»¶
echo "=== æœ€è¿‘äº‹ä»¶ ==="
kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail-10
```

### æ€§èƒ½ç›‘æ§

```bash
# å®‰è£…Prometheusç›‘æ§
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install grafana prometheus-community/grafana -n monitoring

# è®¿é—®æŒ‡æ ‡
kubectl port-forward <pod> 9090:9090
curl http://localhost:9090/metrics
```

## ğŸš¨ åº”æ€¥å¤„ç†

### å¿«é€Ÿæ¢å¤

```bash
# é‡å¯æ‰€æœ‰ç»„ä»¶
kubectl rollout restart deployment -n llm-d

# å¼ºåˆ¶é‡æ–°è°ƒåº¦
kubectl delete pods --field-selector=status.phase!=Running -n llm-d

# æ¸…ç†ç¼“å­˜ï¼ˆå¦‚ç¼“å­˜æŸåï¼‰
kubectl exec -it <kv-cache-pod> -- rm -rf /cache/*
```

### æœåŠ¡é™çº§

```yaml
fallback:
  enabled: true
  conditions:
    - error_rate > 0.1
    - response_time_p99 > 30s
  actions:
    - switch_to_simple_routing
    - disable_caching
    - reduce_batch_size
```

### å®¹é‡æ‰©å±•

```bash
# è‡ªåŠ¨æ‰©å®¹
kubectl autoscale deployment gaie-xxx-epp \
  --cpu-percent=70 \
  --min=2 \
  --max=10 \
  -n llm-d

# æ‰‹åŠ¨æ‰©å®¹
kubectl scale deployment ms-xxx-decode --replicas=5 -n llm-d
```

## ğŸ“ è·å–å¸®åŠ©

### ä¿¡æ¯æ”¶é›†

```bash
# æ”¶é›†è¯Šæ–­ä¿¡æ¯
kubectl get all -n llm-d > cluster-state.txt
kubectl describe pods -n llm-d >> cluster-state.txt
kubectl logs --all-containers=true -n llm-d >> logs.txt
kubectl get events -n llm-d >> events.txt

# æ‰“åŒ…å‘é€
tar -czf llm-d-diagnostics.tar.gz cluster-state.txt logs.txt events.txt
```

### æ”¯æŒæ¸ é“

- **GitHub Issues**: https://github.com/llm-d/llm-d/issues
- **Slack**: #llm-d-community
- **ä¼ä¸šæ”¯æŒ**: support@llm-d.io (2å°æ—¶å“åº”)

## ğŸ”„ é¢„é˜²æ€§ç»´æŠ¤

### å¥åº·æ£€æŸ¥

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: llm-d-health-check
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: health-check
            image: curlimages/curl
            command:
            - /bin/sh
            - -c
            - |
              curl -f http://gateway:8080/health || exit 1
```

### å‘Šè­¦é…ç½®

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: llm-d-alerts
spec:
  groups:
  - name: llm-d
    rules:
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
      for: 2m
    - alert: LowCacheHitRatio
      expr: cache_hit_ratio < 0.1
      for: 5m

# llm-d æ¶æ„æŒ‡å—

> **llm-d v0.4.0** - åˆ†å¸ƒå¼AIæ¨ç†æœåŠ¡æ ˆæ¶æ„ä¸å®æˆ˜æŒ‡å—

## ï¿½ å¿«é€Ÿå¼€å§‹

### æ ¸å¿ƒç‰¹æ€§
- **æ™ºèƒ½æ¨ç†è°ƒåº¦**: EPP + vLLM + Gatewayï¼ŒP90å»¶è¿Ÿæ”¹å–„3å€
- **å¤šæ¨¡å‹ç»Ÿä¸€**: Body-Based RoutingåŠ¨æ€è·¯ç”±ï¼Œæ— éœ€è·¯å¾„åˆ†ç¦»
- **é«˜çº§ç¼“å­˜**: KVç¼“å­˜æ„ŸçŸ¥è°ƒåº¦ï¼Œåˆ†å±‚å­˜å‚¨ç®¡ç†
- **ä¸€é”®éƒ¨ç½²**: Helmwaveç»Ÿä¸€éƒ¨ç½²ï¼Œæ”¯æŒGPU/CPU/TPU/XPU

### å¿«é€Ÿéƒ¨ç½²
```bash
# æ­¥éª¤1: æ¨¡å‹å‡†å¤‡
cd workspace/llm
kubectl apply -k .
kubectl exec -it llm-dev-xxx -n llm -- bash
modelscope download --model deepseek-ai/DeepSeek-V3.1 --local_dir /models/deepseek-ai/DeepSeek-V3.1
modelscope download --model Qwen/Qwen3-VL-235B-A22B-Instruct --local_dir /models/Qwen/Qwen3-VL-235B-A22B-Instruct

# æ­¥éª¤2: ä¸€é”®éƒ¨ç½²ï¼ˆæ¨èï¼‰
cd workspace/llm-d-0.4.0
helmwave up --build
```

## ğŸ“‹ éƒ¨ç½²é€‰æ‹©

| åœºæ™¯ | æ¨¡å¼ | å‘½ä»¤ | ä¼˜åŠ¿ |
|------|------|------|------|
| **æ–°æ‰‹å…¥é—¨** | æ™ºèƒ½æ¨ç†è°ƒåº¦ | `helmwave up --build` | ç®€å•å¿«é€Ÿï¼Œé»˜è®¤ä¼˜åŒ– |
| **ç”Ÿäº§æœåŠ¡** | æ™ºèƒ½è°ƒåº¦ + BBR | `helmwave up --build` + BBRé…ç½® | å¤šæ¨¡å‹æ”¯æŒï¼Œé«˜å¯ç”¨ |
| **å¤§æ¨¡å‹** | é¢„å¡«å……/è§£ç åˆ†ç¦» | `helmfile apply -e prefill && helmfile apply -e decode` | é•¿æç¤ºä¼˜åŒ–ï¼ŒTTFTæå‡ |
| **MoEæ¨¡å‹** | å®½ä¸“å®¶å¹¶è¡Œ | `helm install wide-ep-pool ...` | æœ€å¤§ååï¼Œä¸“å®¶å¹¶è¡Œ |

## ğŸ—ï¸ æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | åŠŸèƒ½ | å…³é”®ç‰¹æ€§ | ç‰ˆæœ¬ |
|------|------|----------|------|
| **Inference Scheduler** | æ™ºèƒ½è·¯ç”±å†³ç­– | æ’ä»¶åŒ–è°ƒåº¦ï¼Œç¼“å­˜æ„ŸçŸ¥ | v0.3.2 |
| **Model Service** | æ¨¡å‹æœåŠ¡ç®¡ç† | å£°æ˜å¼éƒ¨ç½²ï¼Œå¤šç¡¬ä»¶æ”¯æŒ | v0.3.8 |
| **Body-Based Router** | å¤šæ¨¡å‹åŠ¨æ€è·¯ç”± | è¯·æ±‚ä½“è§£æï¼Œç»Ÿä¸€è·¯å¾„ | v1.0.0 |
| **KV Cache Manager** | ç¼“å­˜ç®¡ç† | åˆ†å±‚å­˜å‚¨ï¼Œè·¨èŠ‚ç‚¹åè°ƒ | v0.3.0 |

## ğŸ”§ å®æˆ˜é…ç½®

### æ™ºèƒ½æ¨ç†è°ƒåº¦é…ç½®
```yaml
# EPPæ’ä»¶é…ç½®
plugins:
  - type: low-queue-filter
  - type: kv-cache-scorer
  - type: prefix-cache-scorer
    parameters:
      hashBlockSize: 64
      maxPrefixBlocksToMatch: 256
```

### æ¨¡å‹æœåŠ¡é…ç½®
```yaml
# DeepSeek V3.1 (SGLang + TP8 + FP8)
modelArtifacts:
  uri: "pvc://llm-model/deepseek-ai/DeepSeek-V3.1"
  name: DeepSeek-V3.1
resources:
  requests:
    nvidia.com/gpu: 8
    memory: 2Ti

# Qwen3-VL-235B (vLLM + 2Pod4G + KVç¼“å­˜)
modelArtifacts:
  uri: "pvc://llm-model/Qwen/Qwen3-VL-235B-A22B-Instruct"
  name: Qwen3-VL-235B-A22B-Instruct
resources:
  requests:
    nvidia.com/gpu: 4
    memory: 1Ti
```

### å¤šæ¨¡å‹è·¯ç”±é…ç½®
```yaml
# BBR + EPP é“¾å¼é…ç½®
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
spec:
  configPatches:
    - applyTo: HTTP_FILTER
      patch:
        operation: INSERT_FIRST  # BBRç¬¬ä¸€
        value:
          name: envoy.filters.http.ext_proc
          processing_mode:
            request_body_mode: FULL_DUPLEX_STREAMED
    - applyTo: HTTP_FILTER
      patch:
        operation: INSERT_AFTER   # EPPç¬¬äºŒ
        value:
          name: envoy.filters.http.ext_proc
          processing_mode:
            request_body_mode: BUFFERED

# HTTPRouteåŒ¹é…
matches:
  - headers:
    - type: Exact
      name: X-Gateway-Model-Name
      value: "Model-Name"
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

| æŒ‡æ ‡ | ä¼˜ç§€ | å¯æ¥å— | éœ€ä¼˜åŒ– |
|------|------|----------|----------|
| **TTFT** | <1s | <3s | >5s |
| **TPOT** | <50ms/token | <100ms/token | >200ms/token |
| **ç¼“å­˜å‘½ä¸­ç‡** | >30% | >10% | <10% |
| **GPUåˆ©ç”¨ç‡** | >80% | >60% | <40% |

## ğŸ› ï¸ æ•…éšœæ’æŸ¥

### å¿«é€Ÿè¯Šæ–­
```bash
# æ£€æŸ¥éƒ¨ç½²çŠ¶æ€
kubectl get pods -n llm-d
kubectl get svc -n llm-d

# æ£€æŸ¥å…³é”®æ—¥å¿—
kubectl logs -f <epp-pod> | grep "Request handled"
kubectl logs -f <vllm-pod> | grep -i error

# æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
kubectl port-forward <pod> 9090:9090
curl localhost:9090/metrics | grep endpoint_picker
```

### å¸¸è§é—®é¢˜
| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| **Pod CrashLoopBackOff** | ConfigMapé…ç½®é”™è¯¯ | æ£€æŸ¥æ’ä»¶é…ç½®è¯­æ³• |
| **è·¯ç”±å¤±è´¥** | HTTPRouteåŒ¹é…è§„åˆ™é”™è¯¯ | éªŒè¯headersåŒ¹é…é…ç½® |
| **ç¼“å­˜å‘½ä¸­ç‡ä½** | prefix-cache-scoreræœªå¯ç”¨ | åˆ‡æ¢åˆ°V2æ’ä»¶ |
| **OOMé”™è¯¯** | GPUå†…å­˜ä¸è¶³ | è°ƒæ•´gpu-memory-utilizationæˆ–å‡å°‘batch_size |

## ğŸ“š æ–‡æ¡£ç»“æ„

### ğŸ—ï¸ æ¶æ„è®¾è®¡
- **[01-overview.md](architecture/01-overview.md)** - æ•´ä½“æ¶æ„æ€»è§ˆ
- **[02-dataflow.md](architecture/02-dataflow.md)** - æ•°æ®æµæ¶æ„å’Œå…³é”®è·¯å¾„

### ğŸ”§ æ ¸å¿ƒç»„ä»¶
- **[01-inference-scheduler.md](components/01-inference-scheduler.md)** - Inference Schedulerè¯¦è§£
- **[02-model-service.md](components/02-model-service.md)** - Model Serviceè¯¦è§£
- **[03-body-based-routing.md](components/03-body-based-routing.md)** - Body-Based Routingè¯¦è§£
- **[04-kv-cache-manager.md](components/04-kv-cache-manager.md)** - KV Cache Managerè¯¦è§£
- **[05-inference-sim.md](components/05-inference-sim.md)** - Inference Simulatorè¯¦è§£
- **[06-benchmark-tools.md](components/06-benchmark-tools.md)** - Benchmark Toolsè¯¦è§£

### ğŸš€ éƒ¨ç½²é…ç½®
- **[01-quick-start.md](deployment/01-quick-start.md)** - å¿«é€Ÿå…¥é—¨æŒ‡å—

### ğŸ” æ•…éšœæ’æŸ¥
- **[01-common-issues.md](troubleshooting/01-common-issues.md)** - å¸¸è§æ•…éšœæ’æŸ¥æŒ‡å—

## ğŸ“ è·å–å¸®åŠ©

### ç¤¾åŒºæ”¯æŒ
- GitHub Issues: https://github.com/llm-d/llm-d/issues
- Slack: #llm-d-community
- æ–‡æ¡£: https://llm-d.io/docs

### ä¼ä¸šæ”¯æŒ
- æŠ€æœ¯æ”¯æŒ: support@llm-d.io
- SLA: 2å°æ—¶å“åº”ï¼Œ24å°æ—¶è§£å†³

---

**æ³¨æ„**: æœ¬æŒ‡å—åŸºäºllm-d v0.4.0ç‰ˆæœ¬ç¼–å†™ï¼ŒåŸºäºå®é™…éªŒè¯çš„v1.3.1éƒ¨ç½²ç»éªŒã€‚å»ºè®®é…åˆå®˜æ–¹æ–‡æ¡£ä½¿ç”¨ã€‚

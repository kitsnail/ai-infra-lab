# llm-d 架构总览

## 🏗️ 系统架构

llm-d是一个分布式AI推理服务栈，采用云原生架构设计，支持多模型、多硬件、高并发的智能推理服务。

### 核心设计理念
- **可组合性**: 微服务架构，组件可独立部署和扩展
- **云原生**: 基于Kubernetes，支持声明式配置
- **智能化**: 基于缓存感知的智能路由调度
- **高性能**: 分层缓存、批处理优化、硬件加速

## 📐 整体架构图

```
客户端请求
    ↓
┌─────────────────────────────────────────────────────────────────┐
│                    llm-d 服务栈                           │
├─────────────────────────────────────────────────────────────────┤
│ 请求入口层                                                │
│ ┌──────────────┐ ┌──────────────┐ ┌──────────────────┐ │
│ │ Istio       │ │ HTTPRoute   │ │ Body-Based     │ │
│ │ Gateway     │ │            │ │ Router (BBR)   │ │
│ └──────────────┘ └──────────────┘ └──────────────────┘ │
├─────────────────────────────────────────────────────────────────┤
│ 路由决策层                                                │
│ ┌──────────────────────────────────────────────────────────┐   │
│ │ Inference Scheduler (EPP)                         │   │
│ │ - 插件化调度                                     │   │
│ │ - 缓存感知路由                                     │   │
│ │ - 负载均衡                                       │   │
│ └──────────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────────────┤
│ 推理执行层                                                │
│ ┌──────────────┐ ┌──────────────┐ ┌──────────────────┐ │
│ │ Model       │ │ KV Cache    │ │ vLLM/SGLang   │ │
│ │ Service     │ │ Manager    │ │ Engines       │ │
│ │            │ │            │ │               │ │
│ └──────────────┘ └──────────────┘ └──────────────────┘ │
├─────────────────────────────────────────────────────────────────┤
│ 数据存储层                                                │
│ ┌──────────────┐ ┌──────────────┐ ┌──────────────────┐ │
│ │ PVC存储     │ │ 本地缓存    │ │ 共享存储       │ │
│ │ (模型权重)   │ │ (KV缓存)    │ │ (跨节点同步)   │ │
│ └──────────────┘ └──────────────┘ └──────────────────┘ │
└─────────────────────────────────────────────────────────────────┘
```

## 🔄 请求处理流程

### 标准推理调度模式
```
客户端请求 → Istio Gateway → BBR (可选) → EPP → Model Service → vLLM → 响应
    ↓              ↓            ↓          ↓           ↓
  (HTTP 80)     (解析model)   (智能路由)   (模型管理)   (推理执行)
```

### 核心处理步骤
1. **请求接收**: Gateway接收HTTP请求，解析路由规则
2. **模型识别**: BBR解析请求体，设置`X-Gateway-Model-Name`头部
3. **智能路由**: EPP基于缓存状态、队列深度、GPU利用率选择最优端点
4. **模型服务**: Model Service管理模型生命周期，负责vLLM实例调度
5. **推理执行**: vLLM/SGLang执行实际推理，处理前缀填充和解码
6. **缓存管理**: KV Cache Manager管理分层缓存，支持跨节点共享

## 🧩 核心组件关系

### EPP ↔ Model Service
- **调度决策**: EPP决定请求路由到哪个Model Service实例
- **状态反馈**: Model Service向EPP报告队列深度、缓存状态
- **负载均衡**: 基于实时指标实现智能负载分配

### Model Service ↔ vLLM
- **生命周期管理**: Model Service创建、监控、重启vLLM实例
- **资源分配**: 根据模型大小动态分配GPU/CPU资源
- **健康检查**: 监控vLLM状态，故障自动恢复

### KV Cache Manager ↔ 全栈
- **缓存感知**: 向EPP提供缓存状态信息
- **存储管理**: 管理本地和共享缓存存储
- **跨节点协调**: 实现分布式缓存一致性

## 🎯 部署模式

### 1. 智能推理调度（推荐）
- **适用场景**: 通用推理服务，多模型支持
- **组件**: Gateway + EPP + Model Service + vLLM
- **优势**: 简单部署，默认优化，P90延迟改善3倍

### 2. 预填充/解码分离
- **适用场景**: 大模型长提示，优化TTFT
- **组件**: Prefill服务 + Decode服务 + KV缓存传输
- **优势**: TTFT提升，适合长文本处理

### 3. 宽专家并行
- **适用场景**: MoE模型，最大化吞吐
- **组件**: Expert Router + 专家节点池 + 结果聚合
- **优势**: 专家并行，最高吞吐量

## 📊 性能指标

### 关键性能指标 (KPI)
| 指标 | 优秀 | 可接受 | 优化目标 |
|------|------|----------|----------|
| **TTFT** | <1s | <3s | 首token时间 |
| **TPOT** | <50ms/token | <100ms/token | 生成速度 |
| **缓存命中率** | >30% | >10% | 缓存效率 |
| **GPU利用率** | >80% | >60% | 硬件利用 |

### 系统性能优化
- **缓存优化**: 前缀缓存 + 分层存储，缓存命中率提升至30%+
- **批处理优化**: 连续批处理，吞吐量提升2-5倍
- **路由优化**: 智能调度，P90延迟改善3倍

## 🔧 技术栈

### 核心技术
| 组件 | 技术栈 | 版本 |
|------|--------|------|
| **网关** | Istio + Gateway API | 1.27.1 + 1.3.0 |
| **调度器** | EPP + ExtProc | v0.3.2 |
| **模型服务** | vLLM/SGLang | v0.11.1/v0.5.2 |
| **存储** | PVC + 分布式缓存 | - |
| **编排** | Kubernetes + Helm | >=1.29 + 3.12+ |

### 硬件支持
| 硬件类型 | 支持状态 | 推荐配置 |
|----------|----------|----------|
| **NVIDIA GPU** | ✅ 完全支持 | H100 80GB + NVLink |
| **Intel XPU** | ✅ 完全支持 | Data Center GPU Max |
| **Google TPU** | ✅ 完全支持 | TPU v5e/v5p |
| **CPU** | ✅ 完全支持 | 64核 + 128GB RAM |
| **AWS EKS** | ✅ 优化支持 | EFA加速 |

## 📈 扩展性设计

### 水平扩展
- **Pod副本**: 基于HPA自动扩缩容
- **节点扩展**: 集群容量动态调整
- **缓存分片**: 分布式KV缓存管理

### 垂直扩展
- **GPU配额**: 动态GPU资源分配
- **内存优化**: KV缓存内存管理
- **网络带宽**: 高速互连带宽分配

### 多模型扩展
- **模型隔离**: 每个模型独立资源池
- **动态路由**: 基于模型名称的智能路由
- **独立扩展**: 每个模型独立的扩缩容策略

## 🔒 安全架构

### 网络安全
- **mTLS**: 组件间加密通信
- **网络策略**: 微服务网络隔离
- **访问控制**: RBAC权限管理

### 数据安全
- **传输加密**: HTTPS/TLS加密
- **存储加密**: 敏感数据加密存储
- **审计日志**: 完整的访问审计

## 📋 部署配置

### 资源规划
```yaml
# 标准配置（单模型）
models:
  - name: "Qwen3-VL-235B"
    resources:
      gpu: 8
      memory: "1Ti"
      cpu: "16"
      
# 多模型配置  
models:
  - name: "DeepSeek-V3.1"
    resources:
      gpu: 8
      memory: "2Ti"
  - name: "Qwen3-VL-235B" 
    resources:
      gpu: 4
      memory: "1Ti"
```

### 网络配置
```yaml
# 基础网络
networking:
  gateway: true
  bbr: true
  epp: true
  
# 高级网络
networking:
  istio_version: "1.27.1"
  gateway_api: "1.3.0"
  envoy_filters:
    - body-based-router
    - inference-scheduler
```

## 🎯 使用建议

### 新手入门
1. 从智能推理调度模式开始
2. 使用默认配置，快速验证
3. 逐步添加多模型支持

### 生产环境
1. 启用BBR多模型路由
2. 配置高可用部署（多副本）
3. 集成监控和告警
4. 设置自动扩缩容

### 大规模部署
1. 考虑预填充/解码分离
2. 优化缓存配置
3. 启用分布式缓存
4. 配置专用网络
